module InferenceActions

//import "package://pkg.pkl-lang.org/pkl-swift/pkl.swift@0.2.1#/swift.pkl"

version: String = "1.0.0"
author:  String = "Patrick Sy"
updated: String = "2024-06-13"
actions: Listing<InferenceAction>

//hidden inputPlaceholder: String = "{input}"

// Enum Cases
// ====================================================
// case store:  persist the result for later review
// case stream: stream the result to the frontmost application
// case chat:   start new chat and stream the result
// case mk.ics: create calendar file(s) from the result
typealias Completion = "store"|"stream"|"chat"|"mk.ics"

typealias Service = "ChatGPT"|"Claude"|"Gemini"|"Perplexity"|"Proxy"|"Local LM"

// Represents a specific action that can be triggered with the workflow. 
// Defines an inference task, its area of applicability, and the behaviour 
// before, during and after its execution.
//
// Areas of applicability:
// - for universal action and shortcut triggers
// - for priming sessions (e.g. as translation engine)
// - for snippet triggers [NB: currently not supported]
class InferenceAction {

    // The unique identifier of the action
    identifier: String(!isEmpty)

    // The name of the action.
    name: String(!isEmpty)

    // A description of the action explaining its purpose and function.
    description: String(!isEmpty)

    // An optional (internal) note about the action. May elaborate on the intended use and possible constraints.
    note: String?

    // The service to use for inference. If not set, the configured service will be used.
    service: Service?

    // The name of the model to use for inference. If not set, the configured model will be used.
    modelOverride: String? 

    // Used by prototypes, this property propagates shared [system] definitions that are prepended to the effectively used system prompt.
    hidden systemPreamble: String?

    // The system prompt that specifies the inference task, or that elaborates on the task when amending a prototype with a defined [systemPreamble].
    hidden system: String(!isEmpty || systemPreamble != null)

    // The effectively used system prompt.
    fixed systemPrompt: String = "\(systemPreamble ?? "") \(system)".trim()

    // The prompt to use for inference. The `{input}` placeholder will be replaced with the input text. Sometimes required to produce a coherent answer.
    promptTemplate: String? = """
    Text: {input}
    
    Your answer:
    """

    // Optional keywords to match the action when searching for it in Alfred.
    keywords: String?

    // Credit to the prompt author or reference to the source. Share your prompts!
    contributors: List<String> = List("Patrick Sy")

    // Paste to the frontmost application or just copy to the clipboard.
    paste: Boolean = true

    // Whether to simply paste into the frontmost application, replacing the selection or to include the selection.
    preserveSelection: Boolean(requiresPaste) = false

    // A series of key combos to be dispatched before pasting the result.
    //keycombosBefore: Listing<Keycombo>(requiresPaste)?

    // A series of key combos to be dispatched before evaluating the input.
    // Currently unused.
    keycombosBefore: Listing<Keycombo>?

    // A series of key combos to be dispatched after pasting the result.
    // Currently unused.
    keycombosAfter: Listing<Keycombo>(requiresPaste)?

    // A completion "tool call" to instruct the program on how to proccess the result.
    completion: Completion?

    // Whether to expect the user input to passed in as an argument or if the contents of the pasteboard should be evaluated.
    usePasteboard: Boolean = false

    // Whether the action should be included and displayed in the list filter when sending some content (currently only text) to the workflow's "Universal Action".
    // For example, some action may be tailored to a specific app (see [frontmostApplication]) and intended to only be invoked with a keyboard shortcut or snippet.
    // In that case the action should be only called directly and ignored for general purpose inference tasks.
    isPublic: Boolean = true

    // The bundle identifier of a specific application, e.g. 'com.apple.Notes'. Where set, the action is evaluated iff the frontmost application matches this identifier when calling the action. (Invoke via shortcut only).
    frontmostApplication: String?

    // A complete mirror of `ChatRequest.Options`
    options: RequestOptions?

    hidden requiresPaste = (val) ->
        // TODO: If completion == .stream, then this is required. Or paste will be enforced. 
        if (paste == false && val.ifNonNull((it) -> if (it is Boolean) it else true)) // fail if `it` is true or non-null
            throw("Actions that define a preservation strategy or dispatch subsequent key combos must be set to paste (value is '\(val.toString())').")
        else true


}

class RequestOptions {

    // Sets the size of the context window used to generate the next token. (Default: 2048)
    maxTokens: Int(this >=64)?

    // Modify the likelihood of specified tokens appearing in the completion.
    // Accepts a JSON object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token.
    // logitBias: Map~

    // Whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the content of message.
    // logprobs: Bool?

    // An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. logprobs must be set to true if this parameter is used.
    // topLogprobs: Int?(this >=0 && this <= 20)

    // How many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep n as 1 to minimize costs.
    n: Int = 1

    // Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.
    // [See more information about frequency and presence penalties.](https://platform.openai.com/docs/guides/text-generation/parameter-details)
    presencePenalty: Float(this >= -2.00 && this <= 2.0)?


    // An object specifying the format that the model must output. Compatible with GPT-4 Turbo and all GPT-3.5 Turbo models newer than gpt-3.5-turbo-1106.
    // Setting to { "type": "json_object" } enables JSON mode, which guarantees the message the model generates is valid JSON.
    // Important: when using JSON mode, you must also instruct the model to produce JSON yourself via a system or user message. Without this, the model may generate an unending stream of whitespace until the generation reaches the token limit, resulting in a long-running and seemingly "stuck" request. Also note that the message content may be partially cut off if finish_reason="length", which indicates the generation exceeded max_tokens or the conversation exceeded the max context length.
    responseFormat: OpenAIResponseFormat?
    
    // This feature is in Beta. If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result. Determinism is not guaranteed, and you should refer to the system_fingerprint response parameter to monitor changes in the backend.
    seed: Int?

    // List<String>?
    stop: String? 

    // Note: Will be overriden if a `Completion` is set that requires streaming.
    stream: Boolean?

    // Options for streaming response. Only set this when you set `stream`: true.
    streamOptions: OpenAIStreamOptions?

    // The temperature of the model. Increasing the temperature will make the model answer more creatively.
    // Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.
    // We generally recommend altering this or `top_p` but not both.
    temperature: Float(this >= 0.01 && this <= 2.0)?

    // An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.
    // We generally recommend altering this or `temperature` but not both.
    topP: Float(this >= 0.01 && this <= 2.0)?

    // E.g. Claude 
    // Reduces the probability of generating nonsense. A higher value (e.g. 100) will give more diverse answers, while a lower value (e.g. 10) will be more conservative. (Default: 40)
    topK: Int(this >= 1 && this <= 200)?

    // Sets how far back for the model to look back to prevent repetition. (Default: 64, 0 = disabled, -1 = num_ctx)
    repeatLastN: Int?

    // Sets how strongly to penalize repetitions. A higher value (e.g., 1.5) will penalize repetitions more strongly, while a lower value (e.g., 0.9) will be more lenient. (Default: 1.1)
    repeatPenalty: Float(this >= 0.01 && this <= 3.0)?

}

class Keycombo {

    // TODO: Perhaps map characters to keycodes
    // TODO: Perhaps define a constraint, e.g. must contain at least one modifier key.
    // <https://pkl-lang.org/package-docs/pkl/0.25.3/base/String#codePoints>
    //
    // The sequence to dispatch, e.g. `⇧⌘←`
    sequence: String

    // The number of times to dispatch the key combo.
    count: Int = 1

}

typealias OpenAIResponseFormatOption = "text"|"json_object"

class OpenAIResponseFormat {
    type: OpenAIResponseFormatOption
}

class OpenAIStreamOptions {
    includeUsage: Boolean
}